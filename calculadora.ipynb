{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de calculadora usando o PLY (Python Lex-Yacc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando o ply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ply.lex import lex\n",
    "from ply.yacc import yacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faremos uma calculadora para a seguinte gramatica:\n",
    "\n",
    "S' -> EXPRESSAO <br>\n",
    "EXPRESSAO -> numero OPERACAO numero <br>\n",
    "OPERACAO -> mais <br>\n",
    "OPERACAO -> menos <br>\n",
    "OPERACAO -> multiplicacao <br>\n",
    "OPERACAO -> divisao <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisador Lexico (Lex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O analisador lexico define os tokens que serão usados na linguagem. A definição de Token para o nosso contexto são os terminais da gramática. Os tokens são definidos por duas coisas: A primeira é a uma lista ou tupla com o nome de variavel chamado `tokens`, a segunda é com nomes de variaveis ou por funções do tipo `t_NOME_TOKEN`, mas entre nome de variaveis e funções, independentemente da forma que escolhermos, devemos liga-los a uma expressão regular que define o padrão de cada token. Um detalhe importante é que **a ordem de definição dos tokens (t_NOME_TOKEN) é importante, ele sempre dará preferencia aos tokens definidos primeiro**, por isso é importante definir os tokens com menos expressividade primeiro. A baixo temos os tokens mais basicos que usaremos para o nosso exemplo de calculadora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mais', 'menos', 'multiplicacao', 'divisao')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# nome dos tokens de operadores e constantes\n",
    "reservados = ('mais','menos','multiplicacao','divisao') \n",
    "\n",
    "# expressões regulares para tokens de operadores e constantes \n",
    "\n",
    "t_mais = r'\\+'\n",
    "t_menos = r'-'\n",
    "t_multiplicacao = r'\\*'\n",
    "t_divisao = r'/'\n",
    "\n",
    "reservados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui em baixo vamos definir numeros, como este é o nosso token de maior expressividade temos que defini-lo por ultimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mais', 'menos', 'multiplicacao', 'divisao', 'numero')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def t_numero(t): # aqui definimos o token numero, ele nesse caso converte o valor direto para um inteiro, mas poderia ser um float\n",
    "    r'\\d+'\n",
    "    t.value = int(t.value)\n",
    "    return t\n",
    "\n",
    "t_ignore = ' \\t\\n' # ignora espaços e tabs\n",
    "\n",
    "def t_error(t): # nos dizer qual caractere ilegal e se tem erro\n",
    "    print(\"Caracter ilegal: \", t.value[0])\n",
    "    t.lexer.skip(1)\n",
    "\n",
    "tokens = reservados + ('numero',)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos instanciar o nosso analisador lexico, no inicio vamos deixa-lo com o modo de debug ativado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lex: tokens   = ('mais', 'menos', 'multiplicacao', 'divisao', 'numero')\n",
      "lex: literals = ''\n",
      "lex: states   = {'INITIAL': 'inclusive'}\n",
      "lex: Adding rule t_numero -> '\\d+' (state 'INITIAL')\n",
      "lex: Adding rule t_mais -> '\\+' (state 'INITIAL')\n",
      "lex: Adding rule t_multiplicacao -> '\\*' (state 'INITIAL')\n",
      "lex: Adding rule t_menos -> '-' (state 'INITIAL')\n",
      "lex: Adding rule t_divisao -> '/' (state 'INITIAL')\n",
      "lex: ==== MASTER REGEXS FOLLOW ====\n",
      "lex: state 'INITIAL' : regex[0] = '(?P<t_numero>\\d+)|(?P<t_mais>\\+)|(?P<t_multiplicacao>\\*)|(?P<t_menos>-)|(?P<t_divisao>/)'\n"
     ]
    }
   ],
   "source": [
    "__file__ = 'calculadora.ipynb' # somente para funcionar no jupyter notebook\n",
    "\n",
    "lexer = lex(debug=True) # construção do lexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o debug em true, podemos ver que ele mostra como ele internamente guarda os tokens (como `<t_nome_token>`), podemos ver tambem as expressões regulares assosiadas a ele.\n",
    "\n",
    "Antes de continuarmos, vale a pena relembrar que o PLY nos disponibiliza outras ferramentas como `literals`, aqui somente estará o minimo necessario para fazer o trabalho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisador Sintatico (Yacc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gramatica de Atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gramatica de atributos é uma materia que está bem no final do curso, então se você esta lendo isso e não saber o que é, expliquei brevemente o que é e como funciona. Caso o contrario, pode pular para a proxima seção."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "S → S MAIS A <br>\n",
    "S → A <br>\n",
    "A → 1 <br>\n",
    "A → 0 <br>\n",
    "\n",
    "Se conectar funções a essa gramatica ficaria algo assim (ligação esta sendo representado por ⇛)\n",
    "\n",
    "S → S MAIS A ⇛ `lambda S,MAIS,A: S + A` <br>\n",
    "S → A ⇛ `lambda A: A` <br>\n",
    "A → 1 ⇛ `lambda: 1` <br>\n",
    "A → 0 ⇛ `lambda: 0` <br>\n",
    "\n",
    "Então se executarmos a gramatica de atributos para a entrada `1+1+1+0` teriamos o output igual a 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando a gramatica de atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o PLY a gramatica é definida por funções com `p_REGRA_DA_GRAMATICA`, essas funções precisam de uma regra associada a ela, esta e criada a partir da `DEFINIÇÃO DE DOCUMENTAÇÃO DE FUNÇÃO DO PYTHON que se define entre 3 aspas no INICIO da função`, essas funções tem como parametro um array que a partir do index 1 são os tokens e o retorno de outras regras, o primeiro valor desse array (index 0) é o retorno da regra atual, para ficar mais claro, vamos ver um exemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "def p_INICIO(regras):\n",
    "    '''\n",
    "    INICIO : EXPRESSAO\n",
    "           | numero\n",
    "    '''\n",
    "    regras[0] = regras[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso do codigo acima, ele é equivalente a dizermos:\n",
    "\n",
    "S → EXPRESSAO <br>\n",
    "S → num <br>\n",
    "\n",
    "Então agora vamos analisar a regra abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def p_EXPRESSAO(regras):\n",
    "    '''\n",
    "    EXPRESSAO : numero mais numero\n",
    "              | numero menos numero\n",
    "              | numero multiplicacao numero\n",
    "              | numero divisao numero\n",
    "              | numero\n",
    "    '''\n",
    "    if len(regras) == 2: # checa se é o caso de um número ou uma constante\n",
    "        regras[0] = regras[1]\n",
    "        \n",
    "    else: # checa se é o caso de uma operação\n",
    "        if regras[2] == '+':\n",
    "            regras[0] = regras[1] + regras[3]\n",
    "        elif regras[2] == '-':\n",
    "            regras[0] = regras[1] - regras[3]\n",
    "        elif regras[2] == '*':\n",
    "            regras[0] = regras[1] * regras[3]\n",
    "        elif regras[2] == '/':\n",
    "            regras[0] = regras[1] / regras[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que essa função ficou um pouco grande e com muitas regras, aconselho tomar um tempo lendo ela para entender bem como funciona a gramatica de atributos.\n",
    "\n",
    "Ao longo do trabalho vocês, desenvolvedores do analisador, poderam criar novas regras, inclusive para diminuir regras grandes, então eu abaixo nos quebraremos essa regra em regras menores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_EXPRESSAO(regras):\n",
    "    '''\n",
    "    EXPRESSAO : numero OPERACAO numero\n",
    "    '''\n",
    "    \n",
    "    regras[0] = regras[2](regras[1], regras[3])\n",
    "    \n",
    "\n",
    "def p_OPERACAO(regras):\n",
    "    '''\n",
    "    OPERACAO : mais\n",
    "             | menos\n",
    "             | multiplicacao\n",
    "             | divisao\n",
    "    '''\n",
    "    if regras[1] == '+':\n",
    "        regras[0] = lambda x,y: x+y\n",
    "    elif regras[1] == '-':\n",
    "        regras[0] = lambda x,y: x-y\n",
    "    elif regras[1] == '*':\n",
    "        regras[0] = lambda x,y: x*y\n",
    "    elif regras[1] == '/':\n",
    "        regras[0] = lambda x,y: x/y\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto agora temos mais regras, porem elas estão mais simples e mais faceis de entender, agora vamos tentar rodar nosso analisador lexico e sintatico juntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_error(regras):\n",
    "    print(\"Erro de sintaxe\"+ str(regras))\n",
    "\n",
    "parser = yacc(debug=True) # construção do parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto temos um analisadore lexico e sintatico, agora vamos ver se ele consegue avaliar a expressão `3 * 5`.\n",
    "\n",
    "PS: Ele ta dando esses WARNINGS porque ele não precisa daquele INICIO que definimos no inicio, ele ja gera internamente um estado S, pode-se ver isso atraves do parser.out, que é um arquivo de debug do Yacc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse('23 + 4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
